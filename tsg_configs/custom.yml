caption_model: tsgm3
id: tsgmt1
checkpoint_path: /data/scratch/ec23759/tsgmt1
gpu: 0
# start_from: 4

noamopt: true
noamopt_warmup: 20000
label_smoothing: 0.0

input_json: /data/EECS-YuanLab/COCO/cocotalk.json
input_label_h5: /data/EECS-YuanLab/COCO/cocotalk_label.h5
input_att_dir: /data/EECS-YuanLab/COCO/butd_att/
input_rela_dir: /data/EECS-YuanLab/COCO/coco_img_sg/
cached_tokens: /data/EECS-YuanLab/COCO/coco-train-idxs

seq_per_img: 5
batch_size: 20
batch_size_rl: 10
learning_rate: 0.0003
reduce_on_plateau: false
drop_prob_lm: 0.5 # default 0.5

num_layers: 6
input_encoding_size: 512
rnn_size: 2048

# Transformer config
N_enc: 6
N_dec: 6
d_model: 512
d_ff: 2048
num_att_heads: 8
dropout: 0.1

learning_rate_decay_start: 0
scheduled_sampling_start: -1
save_checkpoint_every: 30000
rp_decay_every: 3000
language_eval: 1
val_images_use: 5000
train_sample_n: 5


self_critical_after: -1
structure_after: 50
structure_until: 150
noamopt_rl: false
learning_rate_rl: 0.00003
# learning_rate_decay_start_rl: -1
reduce_on_plateau_rl: true

# train_sample_n_rl: 5
structure_loss_weight: 1
structure_loss_type: new_self_critical

max_epochs: 150
concat_type: 1
controller: 1


# # rewards
# cider_reward_weight: 1 # default 1
# self_cider_reward_weight: 0 # default 0
# bleu_reward_weight: 0 # default 0
# entropy_reward_weight: 0 # default 0

# # beam size
# beam_size: 3
# train_beam_size: 3
# sc_beam_size: 3